{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL | Q-learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_7dFe5gJ0F"
      },
      "source": [
        "Author: Salma Elbess <br>\n",
        "Email: s-salmahasanelemam@zewailcity.edu.eg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5d5IcKcBP4f"
      },
      "source": [
        "Sources: <br>\n",
        "https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv <br>\n",
        "https://reinforcement-learning4.fun/2019/06/16/gym-tutorial-frozen-lake/ <br>\n",
        "https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake_unslippery%20(Deterministic%20version).ipynb <br>\n",
        "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py <br>\n",
        "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf <br>\n",
        "https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb<br>\n",
        "https://deeplizard.com/learn/video/HGeI30uATws <br>\n",
        "https://deeplizard.com/learn/video/mo96Nqlo1L8 <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nqs8ulozBTl"
      },
      "source": [
        "#Frozen lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6fsBf2rEHe5"
      },
      "source": [
        "##Problem description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUd901MYy7jJ"
      },
      "source": [
        "   \n",
        "Winter is here. You and your friends were tossing around a frisbee at the\n",
        "park when you made a wild throw that left the frisbee out in the middle of\n",
        "the lake. The water is mostly frozen, but there are a few holes where the\n",
        "ice has melted. If you step into one of those holes, you'll fall into the\n",
        "freezing water. At this time, there's an international frisbee shortage, so\n",
        "it's absolutely imperative that you navigate across the lake and retrieve\n",
        "the disc. However, the ice is slippery, so you won't always move in the\n",
        "direction you intend.<br>\n",
        "The surface is described using a grid like the following<br>\n",
        "       \n",
        "\n",
        ">   **SFFF<br>\n",
        "        FHFH<br>\n",
        "        FFFH<br>\n",
        "        HFFG**\n",
        "\n",
        "\n",
        "\n",
        "*   S : starting point, safe\n",
        "*   F : frozen surface, safe\n",
        "*   H : hole, fall to your doom\n",
        "*   G : goal, where the frisbee is located\n",
        "\n",
        "\n",
        "\n",
        "The episode ends when you reach the goal or fall in a hole.<br>\n",
        "<b>You receive a reward of 1 if you reach the goal, and zero otherwise.<b>\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTNaUKv0mFN5"
      },
      "source": [
        "#import statmenets\n",
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7NotZ0KDbom"
      },
      "source": [
        "##States and Actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUrw4IF3l6Ym"
      },
      "source": [
        "#create the environment \n",
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z0w43492Hp4"
      },
      "source": [
        "**Avalaible actions:** <br>\n",
        "\n",
        "\n",
        "*   LEFT = 0\n",
        "*   DOWN = 1\n",
        "* RIGHT = 2\n",
        "* UP = 3 <br>\n",
        "\n",
        "**Available States:** The state represents the player position on the grid. The player may be on any square on the grid (16 squares = 16 States)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uItrna6Qmjwu",
        "outputId": "816da8e1-cff7-4c11-87d8-611fef15cd0d"
      },
      "source": [
        "n_actions = env.action_space.n #number of available actions\n",
        "n_states = env.observation_space.n #number of possible states\n",
        "\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action space:  Discrete(4)\n",
            "Observation space:  Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEw1y4CgC0c2"
      },
      "source": [
        "##Stochastic Vs. Deterministic Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd3_PGe8CquI"
      },
      "source": [
        "###Stochastic Environment (Slippery)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xktns7CD7Ug-",
        "outputId": "8257a2bf-f2d2-43bf-d77d-5f19f93c4d31"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\")\n",
        "#play without training\n",
        "env.reset()\n",
        "env.render()\n",
        "for i in range(16):\n",
        "    random_action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(\n",
        "       random_action)\n",
        "    env.render()\n",
        "    print(\"reward: \",reward)\n",
        "    print(info)\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 0.3333333333333333}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05N333_wCxTq"
      },
      "source": [
        "###Deterministic Environment (Non Slippery)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_yunc_l8ghO",
        "outputId": "360dfd2a-6c70-4d7a-c505-ca1f29ab6a6a"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\",is_slippery=False)\n",
        "#play without training\n",
        "env.reset()\n",
        "env.render()\n",
        "for i in range(16):\n",
        "    random_action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(\n",
        "       random_action)\n",
        "    env.render()\n",
        "    print(\"reward: \",reward)\n",
        "    print(info)\n",
        "    if done:\n",
        "        break\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 1.0}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 1.0}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 1.0}\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "reward:  0.0\n",
            "{'prob': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-Jms-C7EVjs"
      },
      "source": [
        "##Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8UkVZmAY1WI"
      },
      "source": [
        "Solving the Frozen Lake 4Ã—4 map using Q-learning. the agent is trained and <br>\n",
        "tested in the deterministic environment, so the results are easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM22LsiBEfBW"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\",is_slippery=False)\n",
        "n_actions = env.action_space.n #number of available actions\n",
        "n_states = env.observation_space.n #number of possible states"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qgvewG6m5Sr",
        "outputId": "831dc905-027e-448e-b0b0-6fc5defee3f1"
      },
      "source": [
        "# constructing the Q-table, actions in the horizontal - states in \n",
        "# vertical (n columns = n actions, n rows = n states)\n",
        "\n",
        "q_table = np.zeros((n_states,n_actions))\n",
        "print(q_table)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzYsoH5snafs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe01fa9-8b0d-4823-d2e1-dbbd314a630f"
      },
      "source": [
        "#initialize parameters\n",
        "\n",
        "n_epis = 20000 #number of episodes the agent will play to learn\n",
        "\n",
        "#max number steps the agent can take in one episode, if excedded without the \n",
        "#agent reaches a terminating state the episode will close and it recieves 0 points\n",
        "max_n_steps_per_episode = 99 \n",
        "\n",
        "learning_rate = 0.8 #alpha\n",
        "discount_rate = 0.95 #gamma \n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate - probability of exploration\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.001             # Exponential decay rate for exploration prob\n",
        "\n",
        "#For visualization reasons\n",
        "n_show = 5 #number of epis to - 1\n",
        "epis_step = 4000// (n_show)\n",
        "epis_to_visualize = list(range(0,4000+1,epis_step))\n",
        "print(epis_to_visualize)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 800, 1600, 2400, 3200, 4000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc8xOmTzezJQ"
      },
      "source": [
        "See how the agent changes its behavior from exploration to exploitation as it <br>is being trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz2S9p_ds7aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc6aebd-d04c-46e4-e1f2-25a82e59abe2"
      },
      "source": [
        "#List of rewards\n",
        "all_epis_rewards = []\n",
        "\n",
        "for epis in range(n_epis):\n",
        "\n",
        "    #for each episode\n",
        "\n",
        "    #reset state\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    current_epis_reward = 0\n",
        "    step = 0\n",
        "    if epis in epis_to_visualize:\n",
        "        print(\"----------------------\")\n",
        "        print(\"Episode \", epis +1)\n",
        "        print(\"Exploration rate: \",epsilon)\n",
        "\n",
        "    #iterate steps\n",
        "    for step in range(max_n_steps_per_episode):\n",
        "        \n",
        "        \n",
        "        \n",
        "        #for each time step\n",
        "        # Exploration-exploitation trade-off\n",
        "        # Take new action\n",
        "        # Update Q-table\n",
        "        # Set new state\n",
        "        # Add new reward   \n",
        "\n",
        "        #1. Exploration-exploitation trade-off - choose an action\n",
        "\n",
        "        #1.1 generate random number\n",
        "        random_num = random.uniform(0,1)\n",
        "\n",
        "        if random_num > epsilon:\n",
        "            #1.2 if random number > epsilon -> exploitation\n",
        "            # Highest q value for the state\n",
        "            action = np.argmax(q_table[state,:])\n",
        "        else:\n",
        "            #1.3 if random number <= epsilon -> exploration\n",
        "            # select random action \n",
        "            action = env.action_space.sample()\n",
        "        if epis in epis_to_visualize:\n",
        "            env.render()\n",
        "            print(\"action with max q-value for state-action pair: \", np.argmax(q_table[state,:]))\n",
        "            print(\"action Selected by agent: \",action)\n",
        "        #2.Take action\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        #3. update Q table for the state action pair\n",
        "        q_table[state,action] = q_table[state,action]*(1-learning_rate) + learning_rate*(reward + discount_rate*np.max(q_table[new_state,:]))\n",
        "\n",
        "        #4. set the new state\n",
        "        state = new_state\n",
        "\n",
        "        #5. add new reward\n",
        "        current_epis_reward += reward\n",
        "\n",
        "        #check if terminating state\n",
        "        if done:\n",
        "            break #stop the episode - move to next episode\n",
        "        \n",
        "    #after each episode - update exploration rate\n",
        "\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epis) \n",
        "    all_epis_rewards.append(current_epis_reward)\n"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------\n",
            "Episode  1\n",
            "Exploration rate:  1.0\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  0\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  0\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  0\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  0\n",
            "  (Left)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  0\n",
            "action Selected by agent:  2\n",
            "----------------------\n",
            "Episode  801\n",
            "Exploration rate:  0.4552807326425205\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  0\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  0\n",
            "----------------------\n",
            "Episode  1601\n",
            "Exploration rate:  0.21007753033962123\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  3\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "----------------------\n",
            "Episode  2401\n",
            "Exploration rate:  0.09990062945063397\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  1\n",
            "----------------------\n",
            "Episode  3201\n",
            "Exploration rate:  0.05039495670453954\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "----------------------\n",
            "Episode  4001\n",
            "Exploration rate:  0.02815062405161077\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "action with max q-value for state-action pair:  1\n",
            "action Selected by agent:  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "action with max q-value for state-action pair:  2\n",
            "action Selected by agent:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA1cISB8getQ"
      },
      "source": [
        "**Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEuZype1KZyf",
        "outputId": "fd2caf58-7f4a-4485-ff21-a93385619697"
      },
      "source": [
        "# Policy Results After Training\n",
        "print(\"Q-table\")\n",
        "print()\n",
        "print(q_table)\n",
        "print()\n",
        "\n",
        "print(\"------------------------ \")\n",
        "\n",
        "env.reset()\n",
        "print(\" Environment \")\n",
        "env.render()\n",
        "print()\n",
        "print(\"------------------------ \")\n",
        "print(\"Action selection for each state\")\n",
        "print()\n",
        "print(np.argmax(q_table,axis=1).reshape(4,4))\n",
        "print(\"LEFT = 0 DOWN = 1 RIGHT = 2 UP = 3\")"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q-table\n",
            "\n",
            "[[0.73509189 0.77378094 0.77378094 0.73509189]\n",
            " [0.73509189 0.         0.81450625 0.77378094]\n",
            " [0.77378094 0.857375   0.77378094 0.81450625]\n",
            " [0.81450625 0.         0.77376622 0.77375009]\n",
            " [0.77378094 0.81450625 0.         0.73509189]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.         0.81450625]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.81450625 0.         0.857375   0.77378094]\n",
            " [0.81450625 0.9025     0.9025     0.        ]\n",
            " [0.857375   0.95       0.         0.857375  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.95       0.857375  ]\n",
            " [0.9025     0.95       1.         0.9025    ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "------------------------ \n",
            " Environment \n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "------------------------ \n",
            "Action selection for each state\n",
            "\n",
            "[[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n",
            "LEFT = 0 DOWN = 1 RIGHT = 2 UP = 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcydB4DQgiei"
      },
      "source": [
        "**Testing the trained agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRatlZFxAXyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df66a2c3-8a45-46f8-f948-1c488d202f03"
      },
      "source": [
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_n_steps_per_episode):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(q_table[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "\n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            break\n",
        "        state = new_state"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}